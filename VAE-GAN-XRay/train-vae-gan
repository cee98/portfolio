train_loader, test_loader = dataloader(4)
gen=VAE_GAN().to(device)
discriminator=Discriminator().to(device)
real_batch = next(iter(train_loader))
real_batch = real_batch / 255.0
print(real_batch.size())

load_save_img("training" ,make_grid((real_batch[0]*0.5+0.5).cpu(),4))


#number of training epochs
epochs=20
#learning rate for RMSprop
lr=3e-4
#hyperparameter alpha
alpha=0.1
#hyperparameter gamma for loss function to balance reconstruction loss and GAN loss
gamma=15

criterion=nn.BCELoss().to(device)


#initialise RMSprop optimisers
encoder_optimiser=torch.optim.RMSprop(gen.encoder.parameters(), lr=lr)
decoder_optimiser=torch.optim.RMSprop(gen.decoder.parameters(), lr=lr)
discrim_optimiser=torch.optim.RMSprop(discriminator.parameters(), lr=lr*alpha)

# Initialize a variable to track the lowest GAN loss and set it to a high initial value
lowest_gan_loss = float('inf')

# Directory to save the model with the lowest GAN loss
best_model_dir = '/content/drive/MyDrive/ImagesHands/best_model/'

# Make sure the directory exists, create it if necessary
os.makedirs(best_model_dir, exist_ok=True)


#generates a tensor of random values of shape (4, 128) for fixed noise
z_fixed=Variable(torch.randn((4,128))).to(device)
#contains real images
x_fixed=Variable(real_batch).to(device)

#begin training loop
for epoch in range(epochs):
  prior_loss_list,gan_loss_list,recon_loss_list=[],[],[]
  dis_real_list,dis_fake_list,dis_prior_list=[],[],[]
  for i, data in enumerate(train_loader, 0):
    #determines the batch size of the current data batch
    batch_size = data.size()[0]


    #create tensor of ones with a shape matching the batch size, which is used as a target label for real data
    ones_label=Variable(torch.ones(batch_size,1)).to(device)
    #creates tensor of zeros with a shape matching the batch size, which is used as a target label for fake data
    zeros_label=Variable(torch.zeros(batch_size,1)).to(device)
    #create tensor of zeros with a shape (4, 1), which is used as a target label for discriminator
    zeros_label1=Variable(torch.zeros(4,1)).to(device)
    #clone the data, detach it from computation graph, move to device, and set it to require gradients
    datav = data.clone().detach().to(device, dtype=torch.float).requires_grad_(True)

    #generate mean, log_variance, and reconstructed encoding from the generator
    mean, log_variance, rec_enc = gen(datav)

    #generate random noise z_p
    z_p = Variable(torch.randn(4,128)).to(device)
    #generate data x_p from the decoder
    x_p = gen.decoder(z_p)

    #calculate discriminator output for real data
    output = discriminator(datav)[0]

    #calculate the loss for the discriminator on real data
    errD_real = criterion(output, ones_label)
    dis_real_list.append(errD_real.item())

    #discriminator output for reconstructed encoding
    output = discriminator(rec_enc)[0]

    #calculate the loss for the discriminator on reconstructed encoding
    errD_rec_enc = criterion(output, zeros_label)
    dis_fake_list.append(errD_rec_enc.item())

    #discriminator output for data generated by the decoder
    output = discriminator(x_p)[0]

    #calculate the loss for the discriminator on data generated by the decoder
    errD_rec_noise = criterion(output, zeros_label1)
    dis_prior_list.append(errD_rec_noise.item())

    #calculate the GAN loss
    gan_loss = errD_real + errD_rec_enc + errD_rec_noise
    gan_loss_list.append(gan_loss.item())

    #zero out gradients for the discriminator optimiser
    discrim_optimiser.zero_grad()
    #backpropagate the GAN loss and update the discriminator's parameters
    gan_loss.backward(retain_graph=True)
    discrim_optimiser.step()

    #calculate GAN loss again
    output = discriminator(datav)[0]
    errD_real = criterion(output, ones_label)
    output = discriminator(rec_enc)[0]
    errD_rec_enc = criterion(output, zeros_label)
    output = discriminator(x_p)[0]
    errD_rec_noise = criterion(output, zeros_label1)
    gan_loss = errD_real + errD_rec_enc + errD_rec_noise

    #calculate the feature loss (reconstruction_loss)
    x_l_tilde = discriminator(rec_enc)[1]
    x_l = discriminator(datav)[1]
    reconstruction_loss = ((x_l_tilde - x_l) ** 2).mean()

    #calculate the decoder loss (err_dec)
    err_dec = gamma * reconstruction_loss - gan_loss
    recon_loss_list.append(reconstruction_loss.item())

    #zero out gradients for the decoder optimizer
    decoder_optimiser.zero_grad()

    #backpropagate the decoder loss and update the decoder's parameters
    err_dec.backward()
    decoder_optimiser.step()

    #calculate the feature loss and prior loss for the encoder
    mean, log_variance, rec_enc = gen(datav)
    x_l_tilde = discriminator(rec_enc)[1]
    x_l = discriminator(datav)[1]
    reconstruction_loss = ((x_l_tilde - x_l) ** 2).mean()

    prior_loss = 1 + log_variance - mean.pow(2) - log_variance.exp()
    prior_loss = (-0.5 * torch.sum(prior_loss))/torch.numel(mean.data)
    prior_loss_list.append(prior_loss.item())

    #calculate the encoder loss (err_enc)
    err_enc = prior_loss + 5*reconstruction_loss

    #zero out gradients for the encoder optimizer
    encoder_optimiser.zero_grad()

    # Backpropagate the encoder loss and update the encoder's parameters
    err_enc.backward()
    encoder_optimiser.step()

    # Check if the current GAN loss is the lowest observed so far
    if np.mean(gan_loss_list) < lowest_gan_loss:
        lowest_gan_loss = np.mean(gan_loss_list)

        # Save the current model as the best model
        torch.save(gen.encoder.state_dict(), os.path.join(best_model_dir, 'encoder_best.pt'))
        torch.save(gen.decoder.state_dict(), os.path.join(best_model_dir, 'decoder_best.pt'))
        torch.save(discriminator.state_dict(), os.path.join(best_model_dir, 'discriminator_best.pt'))
        torch.save(encoder_optimiser.state_dict(), os.path.join(best_model_dir, 'encoder_optimiser_best.pt'))
        torch.save(decoder_optimiser.state_dict(), os.path.join(best_model_dir, 'decoder_optimiser_best.pt'))
        torch.save(discrim_optimiser.state_dict(), os.path.join(best_model_dir, 'discrim_optimiser_best.pt'))

  # Move x_fixed
  x_fixed = x_fixed.to(device, dtype=torch.float32)

  #Generate data b from the generator
  b=gen(x_fixed)[2]
  b=b.detach()

  #Generate data c from the decoder
  c=gen.decoder(z_fixed)
  c=c.detach()

  #Save images
  load_save_img('reconstructed_noise_generated_%d.png' % epoch ,make_grid((c*0.5+0.5).cpu(),8))
  load_save_img('image_generated_%d.png' % epoch ,make_grid((b*0.5+0.5).cpu(),8))

  # Print losses at the end of each iteration
  print(f"Epoch [{epoch}/{epochs}] Iteration [{i}/{len(train_loader)}]")
  print(f"Discriminator Real Loss: {np.mean(dis_real_list)}")
  print(f"Discriminator Fake Loss: {np.mean(dis_fake_list)}")
  print(f"Discriminator Prior Loss: {np.mean(dis_prior_list)}")
  print(f"GAN Loss: {np.mean(gan_loss_list)}")
  print(f"Reconstruction Loss: {np.mean(recon_loss_list)}")


plot_loss(prior_loss_list, title1="Prior Loss")
plot_loss(recon_loss_list, title1="Reconstruction Loss")
plot_loss(gan_loss_list, title1="GAN Loss")
