#application of weight initialization for Convolutional and Batch Normalization layers
def weights_init(m):

    """
    Initialize the weights for Convolutional and Batch Normalization layers.

    This function initializes the weights of Convolutional layers with a normal distribution,
    where the mean is set to 0 and the standard deviation is set to 0.02. For Batch Normalization
    layers, it initializes the weight with a normal distribution (mean=1.0, std=0.02) and sets
    the bias to zero.

    Args:
        m (torch.nn.Module): The module whose weights need to be initialized.
    """

    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        #initializes the weights of the Convolutional layer- normal distribution, mean = 0, standard deviation = 0.02.
        # weights of the layer set to random values with this distribution
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

class AutoEncoder(nn.Module):
    def __init__(self, latent_dim=128, hidden_dim=512):
        super(AutoEncoder, self).__init__()
        self.fc1 = nn.Linear(latent_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, latent_dim)
        self.relu = nn.LeakyReLU(0.2)

    def forward(self, z):
        z = self.relu(self.fc1(z))
        z = self.fc2(z)
        return z


class VAE_GAN(nn.Module):

    """
    Variational Autoencoder Generative Adversarial Network (VAE-GAN).

    This class represents a VAE-GAN model, which combines a Variational Autoencoder (VAE)
    and a Generative Adversarial Network (GAN) for generative modeling and data generation.

    Args:
        None

    Attributes:
        encoder (Encoder): The encoder network responsible for encoding input data into
            latent space representations.
        decoder (Decoder): The decoder network responsible for generating data from
            latent space representations.
        discriminator (Discriminator): The discriminator network used in the GAN part of the model.
        auto_encoder (AutoEncoder): Used to process the latent code before decoding.

    Methods:
        forward(x):
        Forward pass through the VAE-GAN model.

        Args:
            x (torch.Tensor): Input data tensor.

        Returns:
            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
                - z_mean (torch.Tensor): Mean of the latent variable z.
                - z_log_variance (torch.Tensor): Logarithm of the variance of the latent variable z.
                - x_recon (torch.Tensor): Reconstructed data generated by the model.
    """
    def __init__(self):
        super(VAE_GAN, self).__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()
        self.discriminator = Discriminator()
        # Add the code processor
        self.auto_encoder = AutoEncoder()
        self.encoder.apply(weights_init)
        self.decoder.apply(weights_init)
        self.discriminator.apply(weights_init)
        self.auto_encoder.apply(weights_init)  # Initialize weights for the code processor

    def forward(self, x):

        """
        Forward pass through the VAE-GAN model.

        Args:
            x (torch.Tensor): Input data tensor.

        Returns:
            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
                - z_mean (torch.Tensor): Mean of the latent variable z.
                - z_log_variance (torch.Tensor): Logarithm of the variance of the latent variable z.
                - x_recon (torch.Tensor): Reconstructed data generated by the model.
        """

        batch_size = x.size()[0]
        #input x through encoder
        #mean and log-variance of the latent variable z
        z_mean, z_log_variance = self.encoder(x)
        # compute standard deviation from the log-variance
        std = z_log_variance.mul(0.5).exp_()

        # sampling epsilon (random noise) from normal distribution
        # generate epsilon from a normal distribution with mean= 0 and std = 1
        epsilon = torch.randn(batch_size, 128).to(device)
        # combine mean and std with epsilon to sample a value for the latent variable z
        z = z_mean + std * epsilon

        # Process the latent code using the auto encoder
        z_processed = self.auto_encoder(z)

        #passes sampled z through the decoder to generate the reconstructed data (x_recon)
        x_recon = self.decoder(z_processed)
        return z_mean, z_log_variance, x_recon
